{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch06/batch_norm_gradient_check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100], output_size=10,\n",
    "                              use_batchnorm=True)\n",
    "\n",
    "x_batch = x_train[:1]\n",
    "t_batch = t_train[:1]\n",
    "\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch06/batch_norm_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.optimizer import SGD, Adam\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 学習データを削減\n",
    "x_train = x_train[:1000]\n",
    "t_train = t_train[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "def __train(weight_init_std):\n",
    "    bn_network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, \n",
    "                                    weight_init_std=weight_init_std, use_batchnorm=True)\n",
    "    network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10,\n",
    "                                weight_init_std=weight_init_std)\n",
    "    optimizer = SGD(lr=learning_rate)\n",
    "    \n",
    "    train_acc_list = []\n",
    "    bn_train_acc_list = []\n",
    "    \n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "    epoch_cnt = 0\n",
    "    \n",
    "    for i in range(1000000000):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = t_train[batch_mask]\n",
    "    \n",
    "        for _network in (bn_network, network):\n",
    "            grads = _network.gradient(x_batch, t_batch)\n",
    "            optimizer.update(_network.params, grads)\n",
    "    \n",
    "        if i % iter_per_epoch == 0:\n",
    "            train_acc = network.accuracy(x_train, t_train)\n",
    "            bn_train_acc = bn_network.accuracy(x_train, t_train)\n",
    "            train_acc_list.append(train_acc)\n",
    "            bn_train_acc_list.append(bn_train_acc)\n",
    "    \n",
    "            print(\"epoch:\" + str(epoch_cnt) + \" | \" + str(train_acc) + \" - \" + str(bn_train_acc))\n",
    "    \n",
    "            epoch_cnt += 1\n",
    "            if epoch_cnt >= max_epochs:\n",
    "                break\n",
    "                \n",
    "    return train_acc_list, bn_train_acc_list\n",
    "\n",
    "\n",
    "# 3.グラフの描画==========\n",
    "weight_scale_list = np.logspace(0, -4, num=16)\n",
    "x = np.arange(max_epochs)\n",
    "\n",
    "for i, w in enumerate(weight_scale_list):\n",
    "    print( \"============== \" + str(i+1) + \"/16\" + \" ==============\")\n",
    "    train_acc_list, bn_train_acc_list = __train(w)\n",
    "    \n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.title(\"W:\" + str(w))\n",
    "    if i == 15:\n",
    "        plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)\n",
    "        plt.plot(x, train_acc_list, linestyle = \"--\", label='Normal(without BatchNorm)', markevery=2)\n",
    "    else:\n",
    "        plt.plot(x, bn_train_acc_list, markevery=2)\n",
    "        plt.plot(x, train_acc_list, linestyle=\"--\", markevery=2)\n",
    "\n",
    "    plt.ylim(0, 1.0)\n",
    "    if i % 4:\n",
    "        plt.yticks([])\n",
    "    else:\n",
    "        plt.ylabel(\"accuracy\")\n",
    "    if i < 12:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        plt.xlabel(\"epochs\")\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch06/hyperparameter_optimization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.util import shuffle_dataset\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 高速化のため訓練データの削減\n",
    "x_train = x_train[:500]\n",
    "t_train = t_train[:500]\n",
    "\n",
    "# 検証データの分離\n",
    "validation_rate = 0.20\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]\n",
    "\n",
    "\n",
    "def __train(lr, weight_decay, epocs=50):\n",
    "    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                            output_size=10, weight_decay_lambda=weight_decay)\n",
    "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
    "                      epochs=epocs, mini_batch_size=100,\n",
    "                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer.test_acc_list, trainer.train_acc_list\n",
    "\n",
    "\n",
    "# ハイパーパラメータのランダム探索======================================\n",
    "optimization_trial = 100\n",
    "results_val = {}\n",
    "results_train = {}\n",
    "for _ in range(optimization_trial):\n",
    "    # 探索したハイパーパラメータの範囲を指定===============\n",
    "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
    "    lr = 10 ** np.random.uniform(-6, -2)\n",
    "    # ================================================\n",
    "\n",
    "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
    "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
    "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
    "    results_val[key] = val_acc_list\n",
    "    results_train[key] = train_acc_list\n",
    "\n",
    "# グラフの描画========================================================\n",
    "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
    "graph_draw_num = 20\n",
    "col_num = 5\n",
    "row_num = int(np.ceil(graph_draw_num / col_num))\n",
    "i = 0\n",
    "\n",
    "for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n",
    "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
    "\n",
    "    plt.subplot(row_num, col_num, i+1)\n",
    "    plt.title(\"Best-\" + str(i+1))\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    if i % 5: plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    x = np.arange(len(val_acc_list))\n",
    "    plt.plot(x, val_acc_list)\n",
    "    plt.plot(x, results_train[key], \"--\")\n",
    "    i += 1\n",
    "\n",
    "    if i >= graph_draw_num:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch06/optimizer_compare_mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.util import smooth_curve\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import *\n",
    "\n",
    "\n",
    "# 0:MNISTデータの読み込み==========\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "max_iterations = 2000\n",
    "\n",
    "\n",
    "# 1:実験の設定==========\n",
    "optimizers = {}\n",
    "optimizers['SGD'] = SGD()\n",
    "optimizers['Momentum'] = Momentum()\n",
    "optimizers['AdaGrad'] = AdaGrad()\n",
    "optimizers['Adam'] = Adam()\n",
    "#optimizers['RMSprop'] = RMSprop()\n",
    "\n",
    "networks = {}\n",
    "train_loss = {}\n",
    "for key in optimizers.keys():\n",
    "    networks[key] = MultiLayerNet(\n",
    "        input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
    "        output_size=10)\n",
    "    train_loss[key] = []    \n",
    "\n",
    "\n",
    "# 2:訓練の開始==========\n",
    "for i in range(max_iterations):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    for key in optimizers.keys():\n",
    "        grads = networks[key].gradient(x_batch, t_batch)\n",
    "        optimizers[key].update(networks[key].params, grads)\n",
    "    \n",
    "        loss = networks[key].loss(x_batch, t_batch)\n",
    "        train_loss[key].append(loss)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print( \"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
    "        for key in optimizers.keys():\n",
    "            loss = networks[key].loss(x_batch, t_batch)\n",
    "            print(key + \":\" + str(loss))\n",
    "\n",
    "\n",
    "# 3.グラフの描画==========\n",
    "markers = {\"SGD\": \"o\", \"Momentum\": \"x\", \"AdaGrad\": \"s\", \"Adam\": \"D\"}\n",
    "x = np.arange(max_iterations)\n",
    "for key in optimizers.keys():\n",
    "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch06/optimizer_compare_naive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from common.optimizer import *\n",
    "\n",
    "\n",
    "def f(x, y):\n",
    "    return x**2 / 20.0 + y**2\n",
    "\n",
    "\n",
    "def df(x, y):\n",
    "    return x / 10.0, 2.0*y\n",
    "\n",
    "init_pos = (-7.0, 2.0)\n",
    "params = {}\n",
    "params['x'], params['y'] = init_pos[0], init_pos[1]\n",
    "grads = {}\n",
    "grads['x'], grads['y'] = 0, 0\n",
    "\n",
    "\n",
    "optimizers = OrderedDict()\n",
    "optimizers[\"SGD\"] = SGD(lr=0.95)\n",
    "optimizers[\"Momentum\"] = Momentum(lr=0.1)\n",
    "optimizers[\"AdaGrad\"] = AdaGrad(lr=1.5)\n",
    "optimizers[\"Adam\"] = Adam(lr=0.3)\n",
    "\n",
    "idx = 1\n",
    "\n",
    "for key in optimizers:\n",
    "    optimizer = optimizers[key]\n",
    "    x_history = []\n",
    "    y_history = []\n",
    "    params['x'], params['y'] = init_pos[0], init_pos[1]\n",
    "    \n",
    "    for i in range(30):\n",
    "        x_history.append(params['x'])\n",
    "        y_history.append(params['y'])\n",
    "        \n",
    "        grads['x'], grads['y'] = df(params['x'], params['y'])\n",
    "        optimizer.update(params, grads)\n",
    "    \n",
    "\n",
    "    x = np.arange(-10, 10, 0.01)\n",
    "    y = np.arange(-5, 5, 0.01)\n",
    "    \n",
    "    X, Y = np.meshgrid(x, y) \n",
    "    Z = f(X, Y)\n",
    "    \n",
    "    # for simple contour line  \n",
    "    mask = Z > 7\n",
    "    Z[mask] = 0\n",
    "    \n",
    "    # plot \n",
    "    plt.subplot(2, 2, idx)\n",
    "    idx += 1\n",
    "    plt.plot(x_history, y_history, 'o-', color=\"red\")\n",
    "    plt.contour(X, Y, Z)\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.plot(0, 0, '+')\n",
    "    #colorbar()\n",
    "    #spring()\n",
    "    plt.title(key)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch06/overfit_dropout.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 過学習を再現するために、学習データを削減\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# Dropuoutの有無、割り合いの設定 ========================\n",
    "use_dropout = True  # Dropoutなしのときの場合はFalseに\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# グラフの描画==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch06/overfit_weight_decay.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 過学習を再現するために、学習データを削減\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# weight decay（荷重減衰）の設定 =======================\n",
    "#weight_decay_lambda = 0 # weight decayを使用しない場合\n",
    "weight_decay_lambda = 0.1\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "# 3.グラフの描画==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch06/weight_init_activation_histogram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "input_data = np.random.randn(1000, 100)  # 1000個のデータ\n",
    "node_num = 100  # 各隠れ層のノード（ニューロン）の数\n",
    "hidden_layer_size = 5  # 隠れ層が5層\n",
    "activations = {}  # ここにアクティベーションの結果を格納する\n",
    "\n",
    "x = input_data\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "\n",
    "    # 初期値の値をいろいろ変えて実験しよう！\n",
    "    w = np.random.randn(node_num, node_num) * 1\n",
    "    # w = np.random.randn(node_num, node_num) * 0.01\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
    "\n",
    "\n",
    "    a = np.dot(x, w)\n",
    "\n",
    "\n",
    "    # 活性化関数の種類も変えて実験しよう！\n",
    "    z = sigmoid(a)\n",
    "    # z = ReLU(a)\n",
    "    # z = tanh(a)\n",
    "\n",
    "    activations[i] = z\n",
    "\n",
    "# ヒストグラムを描画\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    if i != 0: plt.yticks([], [])\n",
    "    # plt.xlim(0.1, 1)\n",
    "    # plt.ylim(0, 7000)\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch06/weight_init_compare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.util import smooth_curve\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "\n",
    "# 0:MNISTデータの読み込み==========\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "max_iterations = 2000\n",
    "\n",
    "\n",
    "# 1:実験の設定==========\n",
    "weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "networks = {}\n",
    "train_loss = {}\n",
    "for key, weight_type in weight_init_types.items():\n",
    "    networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
    "                                  output_size=10, weight_init_std=weight_type)\n",
    "    train_loss[key] = []\n",
    "\n",
    "\n",
    "# 2:訓練の開始==========\n",
    "for i in range(max_iterations):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    for key in weight_init_types.keys():\n",
    "        grads = networks[key].gradient(x_batch, t_batch)\n",
    "        optimizer.update(networks[key].params, grads)\n",
    "    \n",
    "        loss = networks[key].loss(x_batch, t_batch)\n",
    "        train_loss[key].append(loss)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
    "        for key in weight_init_types.keys():\n",
    "            loss = networks[key].loss(x_batch, t_batch)\n",
    "            print(key + \":\" + str(loss))\n",
    "\n",
    "\n",
    "# 3.グラフの描画==========\n",
    "markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}\n",
    "x = np.arange(max_iterations)\n",
    "for key in weight_init_types.keys():\n",
    "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 2.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
